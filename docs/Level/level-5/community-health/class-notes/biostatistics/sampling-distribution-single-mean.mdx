---
sidebar_position: 4
---

# Sampling distribution of a single mean

Dr Mweu

## Sampling distribution

Confidence intervals and hypothesis tests are linked to the concept of sampling distribution.

When different samples of equal size are repeatedly taken from the same population & we repeatedly calculate the statistics (e.g. estimates of **𝜇**(population mean), **𝜎**(population standard deviation) and **𝜋**(population propotion)) for each sample we get **_populations of statistics_** with **_known probability distributions._**

:::tip NOTE
Normal distribution formed by the sample means gotten: the mean of this distribution/sample means will be equal to population mean; as though census was done. However the variance/dispersion of the sample mean is not equal to population variance; it’s > 1/n times smaller than 𝜎2.
:::

The population originally sampled is called the **_parent population/parent distribution_** (from which samples is drawn) while that of the computed statistic is the **_sampling distribution_**

:::tip NOTE
Sampling distribution has a normal distribution curve. Parent distribution can be normal/skewed distribution, mostly positively skewed.
:::

NB: The idea behind estimation is that 𝑁, no. of individuals in the population is very large compared to 𝑛 the no. of individuals in the sample, so that sampling doesn’t affect the probability of choosing a particular sample – means that although we are not sampling with replacement, in terms of probability, it as though we were sampling with replacement

## Sampling distribution of a mean

If all possible samples of a given size, 𝑛, were picked and a 𝑥 (sample mean) calculated for each sample, the population of 𝑥’s would have a normal distribution with a mean equal to the mean of the parent distribution (𝜇) and a variance that is **<sup>1</sup>&frasl;<sub>n</sub>** times smaller than that of the parent distribution i.e. the sampling distribution of sample mean gives normal distribution with a mean = 𝜇 and a **variance** = **<sup>𝜎2</sup>&frasl;<sub>𝑛</sub>**

**Square root of variance divided by n (<sup>&radic;𝜎2 </sup>&frasl;<sub>n</sub>)** is called the **standard error of the mean** (Standard deviation of sample: a measure of dispersion) which measures 𝑛 the variability/dispersion of the mean’s obtained when taking repeated samples of size 𝑛; ie sample mean (recall: 𝜎 measures the variability of the individual 𝑥’s in the population)

As the sample size (𝑛) increases, the **_standard error of the mean of the sample mean 𝑛 decreases_** (n and SEM are inversely related) meaning that mean’s become clustered
more closely to the mean 𝜇 (increasing n; **_more peaked distribution_** and reducing n; flatter distribution with a smaller variance), we get more precise estimates as 𝑛 increases

If **𝑛** (sample size) is large (**𝑛** ≥ 20; we can use Z score, if n < 20 using Z score gives a narrower CI, we use a wider CI given by another score in t distribution), the sampling distribution of sample mean will be normal with mean = 𝜇 & variance = 𝜎2/n even if 𝑋 (parent variable) is not normally distributed this is called the **central limit theorem (CLT)**

<div style={{ display: 'flex', justifyContent: 'center' }}>
  <img src="https://i.imgur.com/v6Itrw5.png" alt="percentiles" />
</div>

## Confidence interval for a single mean

To make inferences about the true population mean 𝜇 we construct a CI.

We accept that the observed sample mean 𝑥 is generally within 1.96 (recall: 𝑍 0.025 = 1.96) standard errors of the true mean 𝜇 so that the interval: `𝑥 ± 1.96 × 𝑆𝐸(𝑥)`; SEM will usually include the true value.

This means that on repeated sampling, 95% of sample means would fall within 1.96 standard errors of the 𝜇 so that the interval:**𝑥 ± 1.96 × 𝑆𝐸(𝑥)** includes 𝝁 approximately 95% of the time (called the 95% CI)

A 99% CI is given by: `𝑥±2.58×𝑆𝐸(𝑥)` (from probability table Z score at 99% is 2.58)

**Example:** The packed cell volume (PCV) was measured in 25 children sampled randomly from children aged 4 yrs living in a large West African village, with the following results: Mean = 34.1 𝑠 = 4.3 Using the 𝑠 as an unbiased estimator of 𝜎 we obtain the 95% CI of:

<detail>

<div style={{ display: 'flex', justifyContent: 'center' }}>
  <img
    src="https://i.imgur.com/v7oUakd.png"
    alt="confidence-interval"
  />
</div>

<strong>
  Interpretation:95% confident that true population mean PCV lies
  between 32.4 and 35.8
</strong>

</detail>

:::warning tip
For n <20; Use the 𝒕 distribution
:::

## Use of t distribution

As the value of 𝜎 is generally unknown (recall: 95% 𝐶𝐼 = 𝑥 ± 1.96 σ/ square root of 𝑛), we have to use 𝑠 as an estimate of 𝜎; it introduces sampling error in calculation. Due to the this error, the interval: 𝑥 ± 1.96 × 𝑠/square root 𝑛 includes 𝜇 less than 95% of the time i.e. the calculated interval is too narrow.

To correct for this we use a multiplying factor larger than 1.96 – makes interval wider and restores confidence level to 95%. The multiplying factor is contained in the 𝑡 𝑑𝑖𝑠𝑡𝑟𝑖𝑏𝑢𝑡𝑖𝑜𝑛

The factor depends on the degrees of freedom (v) used to calculate the sample SD𝑠 (𝑑𝑓 𝑎𝑟𝑒 𝑜𝑛𝑒 𝑙𝑒𝑠𝑠 𝑡h𝑎𝑛 𝑠𝑎𝑚𝑝𝑙𝑒 𝑠𝑖𝑧𝑒 `𝒗 = 𝒏 − 𝟏`

As 𝑛 increases the factor approaches 𝑍<sub>0.025</sub> = 1.96; hence t distribution only needs to be used for 𝑛 < 20 (can be used for both n; < or > 20 since gives a bigger multiplier compared to a Z score (multiplier of 1.96) as long as sample size is less that infinity)

**Example:** The packed cell volume (PCV) was measured in 25 children sampled randomly from children aged 4 yrs living in a large West African village, with the following results: Mean = 34.1 𝑠 = 4.3 Using the 𝑠 as an unbiased estimator of 𝜎 we obtain the 95% CI of: (Use the t distribution)

<detail>
In the PCV example, 𝑣 = 25 − 1 = 24. Using the 𝑡 distribution with 24 𝑑𝑓, the 95% CI is: Alpha divided by 2= 0.025: using this check for multiplier of t at 24 df
at alpha/2 from t distribution table; you get a multiplier of 2.064 (slightly larger than 1.96 hence the CI obtained is slightly wider than using Z score)

<div style={{ display: 'flex', justifyContent: 'center' }}>
  <img src="https://i.imgur.com/xlpHqKs.png" alt="t-test" />
</div>

Interpretation: 95% confidence that true mean PCV in this population lies between 32.3 and 35.9

</detail>

## Significance test/hypothesis test for a single mean

We may wish to test a specific hypothesis about the pop mean 𝜇

**Example:** if data on 4yr children in USA indicate a mean PCV of 37.1 we may test whether our sample data (West African) are consistent with the 𝐻<sub>0</sub>: First state the hypothesis (using population parameters): in this case it’s 2 sided;

- 𝐻<sub>0</sub>: 𝜇 = 𝜇0 = 37.1 (W. African statistics = null value US)
- 𝐻<sub>𝑎</sub>: 𝜇 ≠ 𝜇0

:::warning tip
You only state hypothesis when running hypothesis/significance test. Confidence interval & hypothesis test are complementary. Both lead to similar conclusion.
:::

One approach to test the hypothesis is to see whether the 95% CI includes the hypothesized value (37.1) – it doesn’t (compare 32.3- 35.9; some evidence against the 𝐻0)

More objectively we use a _significance test_ and examine the _𝑃-value_ (run the test)

N/B: Every significance test has the difference between what is to be compared in the numerator (eg. the difference between the two means) and denominator is SEM

<div style={{ display: 'flex', justifyContent: 'center' }}>
  <img src="https://i.imgur.com/9GzCRgE.png" alt="z-test" />
</div>

From the 𝑍 𝑡𝑎𝑏𝑙𝑒𝑠 we get the 𝑃-value using the Z score of 3.49 (ignore the negative): Read off; 0.00024 (a probability on tail); remember rejection region is in the 5% area. 2 × 0.00024 = 𝟎.𝟎𝟎𝟎𝟒𝟖 (note this is less than 0.05; reject) (NB: 𝑃-value is normally one-tailed so multiply the resulting probability by 2)

Interpretation: The data provide strong evidence (i.e p value is 0.00048) against 𝑯<sub>𝟎</sub> hence the mean PCV in 4yr old children in the West African village is different from that of children of the same age in the USA (we are rejecting the null hypothesis)

If 𝑛 < 20 then 𝑡 𝑑𝑖𝑠𝑡𝑟𝑖𝑏𝑢𝑡𝑖𝑜𝑛 is more appropriate:

<div style={{ display: 'flex', justifyContent: 'center' }}>
  <img src="https://i.imgur.com/otl0vKd.png" alt="t-test" />
</div>

From 𝑡 𝑡𝑎𝑏𝑙𝑒𝑠 3.49 falls between 3.467 & 3.745 and df is (25-1 = 24) hence: read off value between 0.001 and 0.0005: to get p value; (0.001 x 2), (0.0005 x 2), thus 𝟎. 𝟎𝟎𝟏 < 𝑷 < 𝟎.𝟎𝟎𝟐 (> probability of 3.467 and < 3.745): the p value lies in rejection region (< 0.05)

## Sampling distribution of a proportion

Example: In a survey of 335 men attending a health centre in Guilford (UK), 127 (37.9%) men said they were current smokers How we can we use the result from the above sample to say something about the population which it represents? – we use the concept of sampling distribution.  
Suppose we repeatedly took a new sample of 335 men from this health centre (assuming a large no. of men are registered there) and calculated the proportion who smoked & then created a histogram of these values – histogram would represent the sampling distribution of the proportion:

<div style={{ display: 'flex', justifyContent: 'center' }}>
  <img src="https://i.imgur.com/6UyxSOq.png" alt="proportion" />
</div>

In practice we only conduct one survey from which to estimate 𝑝. Is 𝑝 close to **𝜋** (population proportion) or is it very different from **𝜋**?

In any random sample there’s some sampling variation in 𝑝 (sampling error) so that the larger the 𝑛 the smaller the sampling variation

The sampling variation of a proportion is described by its **standard error of the proportion**:

<div style={{ display: 'flex', justifyContent: 'center' }}>
  <img src="https://i.imgur.com/Fs34Xap.png" alt="proportion" />
</div>

The 𝑆𝐸 of a proportion is a measure of how far our observed proportion **𝑝** differs from the true pop proportion **𝜋**

In the previous UK example, the estimated 𝑆𝐸 of the proportion of smokers is:

<div style={{ display: 'flex', justifyContent: 'center' }}>
  <img src="https://i.imgur.com/x8JWeBl.png" alt="proportion" />
</div>

To estimate the confidence interval of possible values within which the true pop proportion 𝜋 lies we compute the CI: (conditions to use Z score should be met)

<div style={{ display: 'flex', justifyContent: 'center' }}>
  <img src="https://i.imgur.com/S2u5Dlh.png" alt="proportion" />
</div>

Interpretation: We are 95% confident that the true percentage of smokers in Guilford, UK lies between 32.7% and 43.1%

### Significance test/hypothesis test for a single proportion

For this 𝑍 test, again the conditions below must be satisfied: 𝑛𝑝 ≥ 5 and 𝑛(1−𝑝) ≥ 5 ( for a Z score to be used for a proportion these conditions must be met; z-test for a single proportion)

<div style={{ display: 'flex', justifyContent: 'center' }}>
  <img
    src="https://i.imgur.com/XSvsY5h.png"
    alt="z-test for a single proportion"
  />
</div>
