---
sidebar_position: 3
---

# Statistical Inference

Dr Mweu

**Statistical inference** is the procedure whereby conclusions about a population are made based on findings from a sample obtained from the population. Since it’s often difficult to measure every individual in the population, samples are taken and inferences drawn from them about the population.

Two measures of statistical inference:

1. **Confidence intervals:** gives an estimated range of values which is likely to include an unknown population parameter, the estimated range being calculated from a set of sample data

2. **Hypothesis tests:** test whether there’s sufficient evidence in a sample of data to infer that a certain condition is true for the entire population (to say something about a population parameter)  These two measures are linked to the concept of sampling distribution

## Confidence intervals

A confidence interval is a pair of numerical values defining an interval, which with a specified degree of confidence includes the parameter being estimated

If we construct a CI for the **population mean 𝜇** with a value for the lower confidence limit (𝐿𝐶𝐿) and a value for the upper confidence limit (𝑈𝐶𝐿) **at the 95% degree of confidence, we can say that we are 95% certain that this CI encloses the true value of the population mean**

## Hypothesis testing

In hypothesis testing we state that _we will reject a certain hypothesis only if there is a 5% or less chance/probability that it is true (if probability is **</= 5%**; **reject**)_

<div style={{ display: 'flex', justifyContent: 'center' }}>
  <img src="https://i.imgur.com/BK4Y66f.png" alt="percentiles" />
</div>

**Hypothesis testing**

### Null hypothesis

Frequently, there’s an expected/natural/true value for a parameter– called the **null value (Ie. actually weigh everyone/do census).**

In hypothesis testing we assess whether the statistic computed from a sample is consistent with the null value.

If there’s consistency then the statistic will be considered equal to the null value (note numerical value is not the same as statistical value) except for **_sampling & measurement errors_**

The argument that there’s consistency between the statistic and the null value is called the null hypothesis; denoted by H0. The 𝐻<sub>0</sub> can therefore be written as:

`𝑯𝟎:𝝁 = 𝝁𝟎` **(statistics = parameter value)**

:::tip Null hypothesis
The assumption that there is no relationship between two measured variables (e.g., the exposure and the outcome) or no significant difference between two studied populations. Statistical tests are used to either reject or accept this hypothesis.
:::

### Alternative hypothesis

Is the opposite of the 𝐻<sub>0</sub>; the assertion that the null value is inconsistent/different with the statistic – is denoted by 𝐻<sub>𝑎</sub>. 𝐻<sub>𝑎</sub> states that the parameter is not equal to, is greater than, or less than the null value

Therefore can be expressed as:

- **𝐻<sub>𝑎</sub>: 𝜇 ≠ 𝜇0 (𝑡𝑤𝑜 𝑠𝑖𝑑𝑒𝑑)**
- **𝐻<sub>𝑎</sub>: 𝜇 > 𝜇0 (𝑜𝑛𝑒 𝑠𝑖𝑑𝑒𝑑)**
- **𝐻<sub>𝑎</sub>: 𝜇 < 𝜇0 (𝑜𝑛𝑒 𝑠𝑖𝑑𝑒𝑑)**

The choice of the **𝐻<sub>𝑎</sub>** will affect the way we conduct the test.

We choose an **𝐻<sub>𝑎</sub>** based on the prior knowledge we have about possible values.

:::tip alternative hypothesis
The assumption that there is a relationship between two measured variables (e.g., the exposure and the outcome) or a significant difference between two studied populations.
:::

#### Two-sided test

We are testing whether 𝜇 is, or is not equal to a specified value 𝜇0. We have **no strong opinion** whether 𝜇 is greater/less than 𝜇0 and we state:

- **𝐻<sub>0</sub>:𝜇 = 𝜇0**
- **𝐻<sub>𝑎</sub>:𝜇 ≠ 𝜇0**

##### One-sided test

We are testing 𝜇 to be greater/less than a given value 𝜇0. We need prior knowledge/an opinion that 𝜇 is on a particular side of 𝜇0 (either less than or greater than). We restate the hypothesis as:

- **𝐻<sub>0</sub>:𝜇 ≥ 𝜇0**
- **𝐻<sub>𝑎</sub>:𝜇 < 𝜇0** 𝑶𝑹
- **𝐻<sub>0</sub>:𝜇 ≤ 𝜇0**
- **𝐻<sub>𝑎</sub>:𝜇 > 𝜇0**

### Significance level (at what level can we reject or accept an hypothesis)

Refers to the null hypothesis.

When the probability (𝑃) that the statistic is consistent with the null value becomes too small, we say that the statistic is **_significantly different from the null value_** and hence we reject **𝐻<sub>0</sub>: 𝜇 = 𝜇0**

How small must 𝑃 be for us to reject the 𝐻<sub>0</sub>? – usually 0.05 (5%) is used

:::tip
The null hypothesis is rejected when there is a relationship between two measured variables (P value < 0.05).
The null hypothesis is accepted when there is no relationship between two measured variables (P value > 0.05).
:::

## Errors in hypothesis testing

### Type I Error

This is denoted by **𝜶 (taken to be 0.05)** – which is the probability that we will reject the 𝐻<sub>0</sub>: μ = 𝜇0 when the 𝐻<sub>0</sub> was actually correct (rejecting a true null hypothesis).

The probability that the 𝐻<sub>0</sub> is true is the **_𝑷-value._**

:::warning Significance level/Type I error rate
Is the probability of a type 1 error (denoted with “α”). The significance level α is usually set to 0.05 (the lower α, the greater the statistical significance).
:::

More correctly, the 𝑃-value is the **_probability that we would observe a statistic equal to, or more extreme/different, than the null value we have observed if the 𝐻<sub>0</sub> is true_** or the probability that a value in a sample or difference of values in two samples will be observed if the null hypothesis is true

:::info
In type I error, the null hypothesis is rejected when it is actually true and, consequently, the alternative hypothesis is accepted, although the observed effect is actually due to chance (false positive error). It occurs when you have extremely large sample size. Probability becomes really small (< 0.05) hence likely to have type I error.
:::

### Type II Error

This is denoted by **𝜷** – occurs when the 𝐻<sub>0</sub> is accepted when the 𝐻<sub>𝑎</sub> is true (NB: **𝛽 is often set at 0.20**) (Type 2; Accepting a false null hypothesis instead of alternative hypothesis)

This allows us to calculate the **_statistical power of a test 𝟏 − 𝜷_** which is the **_probability of rejecting the 𝐻<sub>0</sub> if 𝑯𝒂 is true._**

Also, the power of a test is the **_ability of the test to detect a real difference when that difference exists and is of a certain magnitude (ability of a test to correctly reject the null hypothesis)._**

For a given sample size 𝑛, **_lowering 𝛼 (say below 0.05; increases the acceptance area on the graph) will increase type II error (𝛽)._**

**_Probability of type II error (𝛽) decreases with increase in 𝑛 (increases power of study)._** In other words, the larger the sample size the lower the type II error.

:::info
The null hypothesis is accepted when it is actually false and, consequently, the alternative hypothesis is rejected even though an observed effect did not occur due to chance (false negative error). Type 2 error rate: the probability of a type 2 error (denoted by “β”). Type 1 errors are inversely related to type 2 errors; The increase of one causes a decrease of the other. It occurs when you have a small sample size. P value becomes > 0.05 hence likely to have type II error.
:::

<div style={{ display: 'flex', justifyContent: 'center' }}>
  <img src="https://i.imgur.com/ohnCnLt.png" alt="percentiles" />
</div>

:::info Statistical power (1-β)
The probability of correctly rejecting the null hypothesis, i.e., the ability to detect a difference between two groups when there truly is a difference. Complementary to the type 2 error rate. Positively correlates with the sample size and the magnitude of the association of interest (e.g., increasing the sample size of a study would increase its statistical power). Positively correlates with measurement accuracy. By convention, most studies aim to achieve 80% statistical power.
:::

> **MNEMONIC**
> “The **A**ccusation is **POS**ted But you **NEG**lect it!” (**type I error (Alpha**) is a false positive
> error and **type II error (Beta)** is **false negative error)**
