---
sidebar_position: 3
---

# Statistical Inference

Dr Mweu

**Statistical inference** is the procedure whereby conclusions about a population are made based on findings from a sample obtained from the population. Since it’s often difficult to measure every individual in the population, samples are taken and inferences drawn from them about the population.

Two measures of statistical inference:

1. **Confidence intervals:** gives an estimated range of values which is likely to include an unknown population parameter, the estimated range being calculated from a set of sample data

2. **Hypothesis tests:** test whether there’s sufficient evidence in a sample of data to infer that a certain condition is true for the entire population (to say something about a population parameter)  These two measures are linked to the concept of sampling distribution

## Confidence intervals

A confidence interval is a pair of numerical values defining an interval, which with a specified degree of confidence includes the parameter being estimated

If we construct a CI for the **population mean 𝜇** with a value for the lower confidence limit (𝐿𝐶𝐿) and a value for the upper confidence limit (𝑈𝐶𝐿) **at the 95% degree of confidence, we can say that we are 95% certain that this CI encloses the true value of the population mean**

## Hypothesis testing

In hypothesis testing we state that _we will reject a certain hypothesis only if there is a 5% or less chance/probability that it is true (if probability is **</= 5%**; **reject**)_

<div style={{ display: 'flex', justifyContent: 'center' }}>
  <img src="https://i.imgur.com/BK4Y66f.png" alt="percentiles" />
</div>

**Hypothesis testing**

### Null hypothesis

Frequently, there’s an expected/natural/true value for a parameter– called the **null value (Ie. actually weigh everyone/do census).**

In hypothesis testing we assess whether the statistic computed from a sample is consistent with the null value.

If there’s consistency then the statistic will be considered equal to the null value (note numerical value is not the same as statistical value) except for **_sampling & measurement errors_**

The argument that there’s consistency between the statistic and the null value is called the null hypothesis; denoted by H0. The 𝐻<sub>0</sub> can therefore be written as:

`𝑯𝟎:𝝁 = 𝝁𝟎` **(statistics = parameter value)**

:::tip Null hypothesis
The assumption that there is no relationship between two measured variables (e.g., the exposure and the outcome) or no significant difference between two studied populations. Statistical tests are used to either reject or accept this hypothesis.
:::

### Alternative hypothesis

Is the opposite of the 𝐻<sub>0</sub>; the assertion that the null value is inconsistent/different with the statistic – is denoted by 𝐻<sub>𝑎</sub>. 𝐻<sub>𝑎</sub> states that the parameter is not equal to, is greater than, or less than the null value

Therefore can be expressed as:

- **𝐻<sub>𝑎</sub>: 𝜇 ≠ 𝜇0 (𝑡𝑤𝑜 𝑠𝑖𝑑𝑒𝑑)**
- **𝐻<sub>𝑎</sub>: 𝜇 > 𝜇0 (𝑜𝑛𝑒 𝑠𝑖𝑑𝑒𝑑)**
- **𝐻<sub>𝑎</sub>: 𝜇 < 𝜇0 (𝑜𝑛𝑒 𝑠𝑖𝑑𝑒𝑑)**

The choice of the **𝐻<sub>𝑎</sub>** will affect the way we conduct the test.

We choose an **𝐻<sub>𝑎</sub>** based on the prior knowledge we have about possible values.

:::tip alternative hypothesis
The assumption that there is a relationship between two measured variables (e.g., the exposure and the outcome) or a significant difference between two studied populations.
:::

#### Two-sided test

We are testing whether 𝜇 is, or is not equal to a specified value 𝜇0. We have **no strong opinion** whether 𝜇 is greater/less than 𝜇0 and we state:

- **𝐻<sub>0</sub>:𝜇 = 𝜇0**
- **𝐻<sub>𝑎</sub>:𝜇 ≠ 𝜇0**

##### One-sided test

We are testing 𝜇 to be greater/less than a given value 𝜇0. We need prior knowledge/an opinion that 𝜇 is on a particular side of 𝜇0 (either less than or greater than). We restate the hypothesis as:

- **𝐻<sub>0</sub>:𝜇 ≥ 𝜇0**
- **𝐻<sub>𝑎</sub>:𝜇 < 𝜇0** 𝑶𝑹
- **𝐻<sub>0</sub>:𝜇 ≤ 𝜇0**
- **𝐻<sub>𝑎</sub>:𝜇 > 𝜇0**

### Significance level (at what level can we reject or accept an hypothesis)

Refers to the null hypothesis.

When the probability (𝑃) that the statistic is consistent with the null value becomes too small, we say that the statistic is **_significantly different from the null value_** and hence we reject **𝐻<sub>0</sub>: 𝜇 = 𝜇0**

How small must 𝑃 be for us to reject the 𝐻<sub>0</sub>? – usually 0.05 (5%) is used

:::tip
The null hypothesis is rejected when there is a relationship between two measured variables (P value < 0.05).
The null hypothesis is accepted when there is no relationship between two measured variables (P value > 0.05).
:::

## Errors in hypothesis testing

### Type I Error

This is denoted by **𝜶 (taken to be 0.05)** – which is the probability that we will reject the 𝐻<sub>0</sub>: μ = 𝜇0 when the 𝐻<sub>0</sub> was actually correct (rejecting a true null hypothesis).

The probability that the 𝐻<sub>0</sub> is true is the **_𝑷-value._**

:::warning Significance level/Type I error rate
Is the probability of a type 1 error (denoted with “α”). The significance level α is usually set to 0.05 (the lower α, the greater the statistical significance).
:::

More correctly, the 𝑃-value is the **_probability that we would observe a statistic equal to, or more extreme/different, than the null value we have observed if the 𝐻<sub>0</sub> is true_** or the probability that a value in a sample or difference of values in two samples will be observed if the null hypothesis is true

:::info
In type I error, the null hypothesis is rejected when it is actually true and, consequently, the alternative hypothesis is accepted, although the observed effect is actually due to chance (false positive error). It occurs when you have extremely large sample size. Probability becomes really small (< 0.05) hence likely to have type I error.
:::

### Type II Error

This is denoted by **𝜷** – occurs when the 𝐻<sub>0</sub> is accepted when the 𝐻<sub>𝑎</sub> is true (NB: **𝛽 is often set at 0.20**) (Type 2; Accepting a false null hypothesis instead of alternative hypothesis)

This allows us to calculate the **_statistical power of a test 𝟏 − 𝜷_** which is the **_probability of rejecting the 𝐻<sub>0</sub> if 𝑯𝒂 is true._**

Also, the power of a test is the **_ability of the test to detect a real difference when that difference exists and is of a certain magnitude (ability of a test to correctly reject the null hypothesis)._**

For a given sample size 𝑛, **_lowering 𝛼 (say below 0.05; increases the acceptance area on the graph) will increase type II error (𝛽)._**

**_Probability of type II error (𝛽) decreases with increase in 𝑛 (increases power of study)._** In other words, the larger the sample size the lower the type II error.

:::info
The null hypothesis is accepted when it is actually false and, consequently, the alternative hypothesis is rejected even though an observed effect did not occur due to chance (false negative error). Type 2 error rate: the probability of a type 2 error (denoted by “β”). Type 1 errors are inversely related to type 2 errors; The increase of one causes a decrease of the other. It occurs when you have a small sample size. P value becomes > 0.05 hence likely to have type II error.
:::

<div style={{ display: 'flex', justifyContent: 'center' }}>
  <img src="https://i.imgur.com/ohnCnLt.png" alt="percentiles" />
</div>

:::info Statistical power (1-β)
The probability of correctly rejecting the null hypothesis, i.e., the ability to detect a difference between two groups when there truly is a difference. Complementary to the type 2 error rate. Positively correlates with the sample size and the magnitude of the association of interest (e.g., increasing the sample size of a study would increase its statistical power). Positively correlates with measurement accuracy. By convention, most studies aim to achieve 80% statistical power.
:::

> **MNEMONIC**
> “The **A**ccusation is **POS**ted But you **NEG**lect it!” (**type I error (Alpha**) is a false positive
> error and **type II error (Beta)** is **false negative error)**

## Sampling distribution

Confidence intervals and hypothesis tests are linked to the concept of sampling distribution.

When different samples of equal size are repeatedly taken from the same population & we repeatedly calculate the statistics (e.g. estimates of **𝜇**(population mean), **𝜎**(population standard deviation) and **𝜋**(population propotion)) for each sample we get **_populations of statistics_** with **_known probability distributions._**

:::tip NOTE
Normal distribution formed by the sample means gotten: the mean of this distribution/sample means will be equal to population mean; as though census was done. However the variance/dispersion of the sample mean is not equal to population variance; it’s > 1/n times smaller than 𝜎2.
:::

The population originally sampled is called the **_parent population/parent distribution_** (from which samples is drawn) while that of the computed statistic is the **_sampling distribution_**

:::tip NOTE
Sampling distribution has a normal distribution curve. Parent distribution can be normal/skewed distribution, mostly positively skewed.
:::

NB: The idea behind estimation is that 𝑁, no. of individuals in the population is very large compared to 𝑛 the no. of individuals in the sample, so that sampling doesn’t affect the probability of choosing a particular sample – means that although we are not sampling with replacement, in terms of probability, it as though we were sampling with replacement

## Sampling distribution of a mean

If all possible samples of a given size, 𝑛, were picked and a 𝑥 (sample mean) calculated for each sample, the population of 𝑥’s would have a normal distribution with a mean equal to the mean of the parent distribution (𝜇) and a variance that is **<sup>1</sup>&frasl;<sub>n</sub>** times smaller than that of the parent distribution i.e. the sampling distribution of sample mean gives normal distribution with a mean = 𝜇 and a **variance** = **<sup>𝜎2</sup>&frasl;<sub>𝑛</sub>**

**Square root of variance divided by n (<sup>&radic;𝜎2 </sup>&frasl;<sub>n</sub>)** is called the **standard error of the mean** (Standard deviation of sample: a measure of dispersion) which measures 𝑛 the variability/dispersion of the mean’s obtained when taking repeated samples of size 𝑛; ie sample mean (recall: 𝜎 measures the variability of the individual 𝑥’s in the population)

As the sample size (𝑛) increases, the **_standard error of the mean of the sample mean 𝑛 decreases_** (n and SEM are inversely related) meaning that mean’s become clustered
more closely to the mean 𝜇 (increasing n; **_more peaked distribution_** and reducing n; flatter distribution with a smaller variance), we get more precise estimates as 𝑛 increases

If **𝑛** (sample size) is large (**𝑛** ≥ 20; we can use Z score, if n < 20 using Z score gives a narrower CI, we use a wider CI given by another score in t distribution), the sampling distribution of sample mean will be normal with mean = 𝜇 & variance = 𝜎2/n even if 𝑋 (parent variable) is not normally distributed this is called the **central limit theorem (CLT)**

<div style={{ display: 'flex', justifyContent: 'center' }}>
  <img src="https://i.imgur.com/v6Itrw5.png" alt="percentiles" />
</div>

## Confidence interval for a single mean

To make inferences about the true population mean 𝜇 we construct a CI.

We accept that the observed sample mean 𝑥 is generally within 1.96 (recall: 𝑍 0.025 = 1.96) standard errors of the true mean 𝜇 so that the interval: `𝑥 ± 1.96 × 𝑆𝐸(𝑥)`; SEM will usually include the true value.

This means that on repeated sampling, 95% of sample means would fall within 1.96 standard errors of the 𝜇 so that the interval:**𝑥 ± 1.96 × 𝑆𝐸(𝑥)** includes 𝝁 approximately 95% of the time (called the 95% CI)

A 99% CI is given by: `𝑥±2.58×𝑆𝐸(𝑥)` (from probability table Z score at 99% is 2.58)

**Example:** The packed cell volume (PCV) was measured in 25 children sampled randomly from children aged 4 yrs living in a large West African village, with the following results: Mean = 34.1 𝑠 = 4.3 Using the 𝑠 as an unbiased estimator of 𝜎 we obtain the 95% CI of:

<detail>

<div style={{ display: 'flex', justifyContent: 'center' }}>
  <img
    src="https://i.imgur.com/v7oUakd.png"
    alt="confidence-interval"
  />
</div>

<strong>
  Interpretation:95% confident that true population mean PCV lies
  between 32.4 and 35.8
</strong>

</detail>

:::warning tip
For n <20; Use the 𝒕 distribution
:::

## Use of t distribution

As the value of 𝜎 is generally unknown (recall: 95% 𝐶𝐼 = 𝑥 ± 1.96 σ/ square root of 𝑛), we have to use 𝑠 as an estimate of 𝜎; it introduces sampling error in calculation. Due to the this error, the interval: 𝑥 ± 1.96 × 𝑠/square root 𝑛 includes 𝜇 less than 95% of the time i.e. the calculated interval is too narrow.

To correct for this we use a multiplying factor larger than 1.96 – makes interval wider and restores confidence level to 95%. The multiplying factor is contained in the 𝑡 𝑑𝑖𝑠𝑡𝑟𝑖𝑏𝑢𝑡𝑖𝑜𝑛

The factor depends on the degrees of freedom (v) used to calculate the sample SD𝑠 (𝑑𝑓 𝑎𝑟𝑒 𝑜𝑛𝑒 𝑙𝑒𝑠𝑠 𝑡h𝑎𝑛 𝑠𝑎𝑚𝑝𝑙𝑒 𝑠𝑖𝑧𝑒 `𝒗 = 𝒏 − 𝟏`

As 𝑛 increases the factor approaches 𝑍<sub>0.025</sub> = 1.96; hence t distribution only needs to be used for 𝑛 < 20 (can be used for both n; < or > 20 since gives a bigger multiplier compared to a Z score (multiplier of 1.96) as long as sample size is less that infinity)

**Example:** The packed cell volume (PCV) was measured in 25 children sampled randomly from children aged 4 yrs living in a large West African village, with the following results: Mean = 34.1 𝑠 = 4.3 Using the 𝑠 as an unbiased estimator of 𝜎 we obtain the 95% CI of: (Use the t distribution)

<detail>
In the PCV example, 𝑣 = 25 − 1 = 24. Using the 𝑡 distribution with 24 𝑑𝑓, the 95% CI is: Alpha divided by 2= 0.025: using this check for multiplier of t at 24 df
at alpha/2 from t distribution table; you get a multiplier of 2.064 (slightly larger than 1.96 hence the CI obtained is slightly wider than using Z score)

<div style={{ display: 'flex', justifyContent: 'center' }}>
  <img src="https://i.imgur.com/xlpHqKs.png" alt="t-test" />
</div>

Interpretation: 95% confidence that true mean PCV in this population lies between 32.3 and 35.9

</detail>

## Significance test/hypothesis test for a single mean

We may wish to test a specific hypothesis about the pop mean 𝜇

**Example:** if data on 4yr children in USA indicate a mean PCV of 37.1 we may test whether our sample data (West African) are consistent with the 𝐻<sub>0</sub>: First state the hypothesis (using population parameters): in this case it’s 2 sided;

- 𝐻<sub>0</sub>: 𝜇 = 𝜇0 = 37.1 (W. African statistics = null value US)
- 𝐻<sub>𝑎</sub>: 𝜇 ≠ 𝜇0

:::warning tip
You only state hypothesis when running hypothesis/significance test. Confidence interval & hypothesis test are complementary. Both lead to similar conclusion.
:::

One approach to test the hypothesis is to see whether the 95% CI includes the hypothesized value (37.1) – it doesn’t (compare 32.3- 35.9; some evidence against the 𝐻0)

More objectively we use a _significance test_ and examine the _𝑃-value_ (run the test)

N/B: Every significance test has the difference between what is to be compared in the numerator (eg. the difference between the two means) and denominator is SEM

<div style={{ display: 'flex', justifyContent: 'center' }}>
  <img src="https://i.imgur.com/9GzCRgE.png" alt="z-test" />
</div>

From the 𝑍 𝑡𝑎𝑏𝑙𝑒𝑠 we get the 𝑃-value using the Z score of 3.49 (ignore the negative): Read off; 0.00024 (a probability on tail); remember rejection region is in the 5% area. 2 × 0.00024 = 𝟎.𝟎𝟎𝟎𝟒𝟖 (note this is less than 0.05; reject) (NB: 𝑃-value is normally one-tailed so multiply the resulting probability by 2)

Interpretation: The data provide strong evidence (i.e p value is 0.00048) against 𝑯<sub>𝟎</sub> hence the mean PCV in 4yr old children in the West African village is different from that of children of the same age in the USA (we are rejecting the null hypothesis)

If 𝑛 < 20 then 𝑡 𝑑𝑖𝑠𝑡𝑟𝑖𝑏𝑢𝑡𝑖𝑜𝑛 is more appropriate:

<div style={{ display: 'flex', justifyContent: 'center' }}>
  <img src="https://i.imgur.com/otl0vKd.png" alt="t-test" />
</div>

From 𝑡 𝑡𝑎𝑏𝑙𝑒𝑠 3.49 falls between 3.467 & 3.745 and df is (25-1 = 24) hence: read off value between 0.001 and 0.0005: to get p value; (0.001 x 2), (0.0005 x 2), thus 𝟎. 𝟎𝟎𝟏 < 𝑷 < 𝟎.𝟎𝟎𝟐 (> probability of 3.467 and < 3.745): the p value lies in rejection region (< 0.05)

## Sampling distribution of a proportion

Example: In a survey of 335 men attending a health centre in Guilford (UK), 127 (37.9%) men said they were current smokers How we can we use the result from the above sample to say something about the population which it represents? – we use the concept of sampling distribution.  
Suppose we repeatedly took a new sample of 335 men from this health centre (assuming a large no. of men are registered there) and calculated the proportion who smoked & then created a histogram of these values – histogram would represent the sampling distribution of the proportion:

<div style={{ display: 'flex', justifyContent: 'center' }}>
  <img src="https://i.imgur.com/6UyxSOq.png" alt="proportion" />
</div>

In practice we only conduct one survey from which to estimate 𝑝. Is 𝑝 close to **𝜋** (population proportion) or is it very different from **𝜋**?

In any random sample there’s some sampling variation in 𝑝 (sampling error) so that the larger the 𝑛 the smaller the sampling variation

The sampling variation of a proportion is described by its **standard error of the proportion**:

<div style={{ display: 'flex', justifyContent: 'center' }}>
  <img src="https://i.imgur.com/Fs34Xap.png" alt="proportion" />
</div>

The 𝑆𝐸 of a proportion is a measure of how far our observed proportion **𝑝** differs from the true pop proportion **𝜋**

In the previous UK example, the estimated 𝑆𝐸 of the proportion of smokers is:

<div style={{ display: 'flex', justifyContent: 'center' }}>
  <img src="https://i.imgur.com/x8JWeBl.png" alt="proportion" />
</div>

To estimate the confidence interval of possible values within which the true pop proportion 𝜋 lies we compute the CI: (conditions to use Z score should be met)

<div style={{ display: 'flex', justifyContent: 'center' }}>
  <img src="https://i.imgur.com/S2u5Dlh.png" alt="proportion" />
</div>

Interpretation: We are 95% confident that the true percentage of smokers in Guilford, UK lies between 32.7% and 43.1%

### Significance test/hypothesis test for a single proportion

For this 𝑍 test, again the conditions below must be satisfied: 𝑛𝑝 ≥ 5 and 𝑛(1−𝑝) ≥ 5 ( for a Z score to be used for a proportion these conditions must be met; z-test for a single proportion)

<div style={{ display: 'flex', justifyContent: 'center' }}>
  <img
    src="https://i.imgur.com/XSvsY5h.png"
    alt="z-test for a single proportion"
  />
</div>
