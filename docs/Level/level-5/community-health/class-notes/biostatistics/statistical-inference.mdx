---
sidebar_position: 3
---

# Statistical Inference

Dr Mweu

**Statistical inference** is the procedure whereby conclusions about a population are made based on findings from a sample obtained from the population. Since itâ€™s often difficult to measure every individual in the population, samples are taken and inferences drawn from them about the population.

Two measures of statistical inference:

1. **Confidence intervals:** gives an estimated range of values which is likely to include an unknown population parameter, the estimated range being calculated from a set of sample data

2. **Hypothesis tests:** test whether thereâ€™s sufficient evidence in a sample of data to infer that a certain condition is true for the entire population (to say something about a population parameter) ï‚· These two measures are linked to the concept of sampling distribution

## Confidence intervals

A confidence interval is a pair of numerical values defining an interval, which with a specified degree of confidence includes the parameter being estimated

If we construct a CI for the **population mean ğœ‡** with a value for the lower confidence limit (ğ¿ğ¶ğ¿) and a value for the upper confidence limit (ğ‘ˆğ¶ğ¿) **at the 95% degree of confidence, we can say that we are 95% certain that this CI encloses the true value of the population mean**

## Hypothesis testing

In hypothesis testing we state that _we will reject a certain hypothesis only if there is a 5% or less chance/probability that it is true (if probability is **</= 5%**; **reject**)_

<div style={{ display: 'flex', justifyContent: 'center' }}>
  <img src="https://i.imgur.com/BK4Y66f.png" alt="percentiles" />
</div>

**Hypothesis testing**

### Null hypothesis

Frequently, thereâ€™s an expected/natural/true value for a parameterâ€“ called the **null value (Ie. actually weigh everyone/do census).**

In hypothesis testing we assess whether the statistic computed from a sample is consistent with the null value.

If thereâ€™s consistency then the statistic will be considered equal to the null value (note numerical value is not the same as statistical value) except for **_sampling & measurement errors_**

The argument that thereâ€™s consistency between the statistic and the null value is called the null hypothesis; denoted by H0. The ğ»<sub>0</sub> can therefore be written as:

`ğ‘¯ğŸ:ğ = ğğŸ` **(statistics = parameter value)**

:::tip Null hypothesis
The assumption that there is no relationship between two measured variables (e.g., the exposure and the outcome) or no significant difference between two studied populations. Statistical tests are used to either reject or accept this hypothesis.
:::

### Alternative hypothesis

Is the opposite of the ğ»<sub>0</sub>; the assertion that the null value is inconsistent/different with the statistic â€“ is denoted by ğ»<sub>ğ‘</sub>. ğ»<sub>ğ‘</sub> states that the parameter is not equal to, is greater than, or less than the null value

Therefore can be expressed as:

- **ğ»<sub>ğ‘</sub>: ğœ‡ â‰  ğœ‡0 (ğ‘¡ğ‘¤ğ‘œ ğ‘ ğ‘–ğ‘‘ğ‘’ğ‘‘)**
- **ğ»<sub>ğ‘</sub>: ğœ‡ > ğœ‡0 (ğ‘œğ‘›ğ‘’ ğ‘ ğ‘–ğ‘‘ğ‘’ğ‘‘)**
- **ğ»<sub>ğ‘</sub>: ğœ‡ < ğœ‡0 (ğ‘œğ‘›ğ‘’ ğ‘ ğ‘–ğ‘‘ğ‘’ğ‘‘)**

The choice of the **ğ»<sub>ğ‘</sub>** will affect the way we conduct the test.

We choose an **ğ»<sub>ğ‘</sub>** based on the prior knowledge we have about possible values.

:::tip alternative hypothesis
The assumption that there is a relationship between two measured variables (e.g., the exposure and the outcome) or a significant difference between two studied populations.
:::

#### Two-sided test

We are testing whether ğœ‡ is, or is not equal to a specified value ğœ‡0. We have **no strong opinion** whether ğœ‡ is greater/less than ğœ‡0 and we state:

- **ğ»<sub>0</sub>:ğœ‡ = ğœ‡0**
- **ğ»<sub>ğ‘</sub>:ğœ‡ â‰  ğœ‡0**

##### One-sided test

We are testing ğœ‡ to be greater/less than a given value ğœ‡0. We need prior knowledge/an opinion that ğœ‡ is on a particular side of ğœ‡0 (either less than or greater than). We restate the hypothesis as:

- **ğ»<sub>0</sub>:ğœ‡ â‰¥ ğœ‡0**
- **ğ»<sub>ğ‘</sub>:ğœ‡ < ğœ‡0** ğ‘¶ğ‘¹
- **ğ»<sub>0</sub>:ğœ‡ â‰¤ ğœ‡0**
- **ğ»<sub>ğ‘</sub>:ğœ‡ > ğœ‡0**

### Significance level (at what level can we reject or accept an hypothesis)

Refers to the null hypothesis.

When the probability (ğ‘ƒ) that the statistic is consistent with the null value becomes too small, we say that the statistic is **_significantly different from the null value_** and hence we reject **ğ»<sub>0</sub>: ğœ‡ = ğœ‡0**

How small must ğ‘ƒ be for us to reject the ğ»<sub>0</sub>? â€“ usually 0.05 (5%) is used

:::tip
The null hypothesis is rejected when there is a relationship between two measured variables (P value < 0.05).
The null hypothesis is accepted when there is no relationship between two measured variables (P value > 0.05).
:::

## Errors in hypothesis testing

### Type I Error

This is denoted by **ğœ¶ (taken to be 0.05)** â€“ which is the probability that we will reject the ğ»<sub>0</sub>: Î¼ = ğœ‡0 when the ğ»<sub>0</sub> was actually correct (rejecting a true null hypothesis).

The probability that the ğ»<sub>0</sub> is true is the **_ğ‘·-value._**

:::warning Significance level/Type I error rate
Is the probability of a type 1 error (denoted with â€œÎ±â€). The significance level Î± is usually set to 0.05 (the lower Î±, the greater the statistical significance).
:::

More correctly, the ğ‘ƒ-value is the **_probability that we would observe a statistic equal to, or more extreme/different, than the null value we have observed if the ğ»<sub>0</sub> is true_** or the probability that a value in a sample or difference of values in two samples will be observed if the null hypothesis is true

:::info
In type I error, the null hypothesis is rejected when it is actually true and, consequently, the alternative hypothesis is accepted, although the observed effect is actually due to chance (false positive error). It occurs when you have extremely large sample size. Probability becomes really small (< 0.05) hence likely to have type I error.
:::

### Type II Error

This is denoted by **ğœ·** â€“ occurs when the ğ»<sub>0</sub> is accepted when the ğ»<sub>ğ‘</sub> is true (NB: **ğ›½ is often set at 0.20**) (Type 2; Accepting a false null hypothesis instead of alternative hypothesis)

This allows us to calculate the **_statistical power of a test ğŸ âˆ’ ğœ·_** which is the **_probability of rejecting the ğ»<sub>0</sub> if ğ‘¯ğ’‚ is true._**

Also, the power of a test is the **_ability of the test to detect a real difference when that difference exists and is of a certain magnitude (ability of a test to correctly reject the null hypothesis)._**

For a given sample size ğ‘›, **_lowering ğ›¼ (say below 0.05; increases the acceptance area on the graph) will increase type II error (ğ›½)._**

**_Probability of type II error (ğ›½) decreases with increase in ğ‘› (increases power of study)._** In other words, the larger the sample size the lower the type II error.

:::info
The null hypothesis is accepted when it is actually false and, consequently, the alternative hypothesis is rejected even though an observed effect did not occur due to chance (false negative error). Type 2 error rate: the probability of a type 2 error (denoted by â€œÎ²â€). Type 1 errors are inversely related to type 2 errors; The increase of one causes a decrease of the other. It occurs when you have a small sample size. P value becomes > 0.05 hence likely to have type II error.
:::

<div style={{ display: 'flex', justifyContent: 'center' }}>
  <img src="https://i.imgur.com/ohnCnLt.png" alt="percentiles" />
</div>

:::info Statistical power (1-Î²)
The probability of correctly rejecting the null hypothesis, i.e., the ability to detect a difference between two groups when there truly is a difference. Complementary to the type 2 error rate. Positively correlates with the sample size and the magnitude of the association of interest (e.g., increasing the sample size of a study would increase its statistical power). Positively correlates with measurement accuracy. By convention, most studies aim to achieve 80% statistical power.
:::

> **MNEMONIC**
> â€œThe **A**ccusation is **POS**ted But you **NEG**lect it!â€ (**type I error (Alpha**) is a false positive
> error and **type II error (Beta)** is **false negative error)**

## Sampling distribution

Confidence intervals and hypothesis tests are linked to the concept of sampling distribution.

When different samples of equal size are repeatedly taken from the same population & we repeatedly calculate the statistics (e.g. estimates of **ğœ‡**(population mean), **ğœ**(population standard deviation) and **ğœ‹**(population propotion)) for each sample we get **_populations of statistics_** with **_known probability distributions._**

:::tip NOTE
Normal distribution formed by the sample means gotten: the mean of this distribution/sample means will be equal to population mean; as though census was done. However the variance/dispersion of the sample mean is not equal to population variance; itâ€™s > 1/n times smaller than ğœ2.
:::

The population originally sampled is called the **_parent population/parent distribution_** (from which samples is drawn) while that of the computed statistic is the **_sampling distribution_**

:::tip NOTE
Sampling distribution has a normal distribution curve. Parent distribution can be normal/skewed distribution, mostly positively skewed.
:::

NB: The idea behind estimation is that ğ‘, no. of individuals in the population is very large compared to ğ‘› the no. of individuals in the sample, so that sampling doesnâ€™t affect the probability of choosing a particular sample â€“ means that although we are not sampling with replacement, in terms of probability, it as though we were sampling with replacement

## Sampling distribution of a mean

If all possible samples of a given size, ğ‘›, were picked and a ğ‘¥ (sample mean) calculated for each sample, the population of ğ‘¥â€™s would have a normal distribution with a mean equal to the mean of the parent distribution (ğœ‡) and a variance that is **<sup>1</sup>&frasl;<sub>n</sub>** times smaller than that of the parent distribution i.e. the sampling distribution of sample mean gives normal distribution with a mean = ğœ‡ and a **variance** = **<sup>ğœ2</sup>&frasl;<sub>ğ‘›</sub>**

**Square root of variance divided by n (<sup>&radic;ğœ2 </sup>&frasl;<sub>n</sub>)** is called the **standard error of the mean** (Standard deviation of sample: a measure of dispersion) which measures ğ‘› the variability/dispersion of the meanâ€™s obtained when taking repeated samples of size ğ‘›; ie sample mean (recall: ğœ measures the variability of the individual ğ‘¥â€™s in the population)

As the sample size (ğ‘›) increases, the **_standard error of the mean of the sample mean ğ‘› decreases_** (n and SEM are inversely related) meaning that meanâ€™s become clustered
more closely to the mean ğœ‡ (increasing n; **_more peaked distribution_** and reducing n; flatter distribution with a smaller variance), we get more precise estimates as ğ‘› increases

If **ğ‘›** (sample size) is large (**ğ‘›** â‰¥ 20; we can use Z score, if n < 20 using Z score gives a narrower CI, we use a wider CI given by another score in t distribution), the sampling distribution of sample mean will be normal with mean = ğœ‡ & variance = ğœ2/n even if ğ‘‹ (parent variable) is not normally distributed this is called the **central limit theorem (CLT)**

<div style={{ display: 'flex', justifyContent: 'center' }}>
  <img src="https://i.imgur.com/v6Itrw5.png" alt="percentiles" />
</div>

## Confidence interval for a single mean

To make inferences about the true population mean ğœ‡ we construct a CI.

We accept that the observed sample mean ğ‘¥ is generally within 1.96 (recall: ğ‘ 0.025 = 1.96) standard errors of the true mean ğœ‡ so that the interval: `ğ‘¥ Â± 1.96 Ã— ğ‘†ğ¸(ğ‘¥)`; SEM will usually include the true value.

This means that on repeated sampling, 95% of sample means would fall within 1.96 standard errors of the ğœ‡ so that the interval:**ğ‘¥ Â± 1.96 Ã— ğ‘†ğ¸(ğ‘¥)** includes ğ approximately 95% of the time (called the 95% CI)

A 99% CI is given by: `ğ‘¥Â±2.58Ã—ğ‘†ğ¸(ğ‘¥)` (from probability table Z score at 99% is 2.58)

**Example:** The packed cell volume (PCV) was measured in 25 children sampled randomly from children aged 4 yrs living in a large West African village, with the following results: Mean = 34.1 ğ‘  = 4.3 Using the ğ‘  as an unbiased estimator of ğœ we obtain the 95% CI of:

<detail>

<div style={{ display: 'flex', justifyContent: 'center' }}>
  <img
    src="https://i.imgur.com/v7oUakd.png"
    alt="confidence-interval"
  />
</div>

<strong>
  Interpretation:95% confident that true population mean PCV lies
  between 32.4 and 35.8
</strong>

</detail>

:::warning tip
For n <20; Use the ğ’• distribution
:::

## Use of t distribution

As the value of ğœ is generally unknown (recall: 95% ğ¶ğ¼ = ğ‘¥ Â± 1.96 Ïƒ/ square root of ğ‘›), we have to use ğ‘  as an estimate of ğœ; it introduces sampling error in calculation. Due to the this error, the interval: ğ‘¥ Â± 1.96 Ã— ğ‘ /square root ğ‘› includes ğœ‡ less than 95% of the time i.e. the calculated interval is too narrow.

To correct for this we use a multiplying factor larger than 1.96 â€“ makes interval wider and restores confidence level to 95%. The multiplying factor is contained in the ğ‘¡ ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘¢ğ‘¡ğ‘–ğ‘œğ‘›

The factor depends on the degrees of freedom (v) used to calculate the sample SDğ‘  (ğ‘‘ğ‘“ ğ‘ğ‘Ÿğ‘’ ğ‘œğ‘›ğ‘’ ğ‘™ğ‘’ğ‘ ğ‘  ğ‘¡hğ‘ğ‘› ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’ ğ‘ ğ‘–ğ‘§ğ‘’ `ğ’— = ğ’ âˆ’ ğŸ`

As ğ‘› increases the factor approaches ğ‘<sub>0.025</sub> = 1.96; hence t distribution only needs to be used for ğ‘› < 20 (can be used for both n; < or > 20 since gives a bigger multiplier compared to a Z score (multiplier of 1.96) as long as sample size is less that infinity)

**Example:** The packed cell volume (PCV) was measured in 25 children sampled randomly from children aged 4 yrs living in a large West African village, with the following results: Mean = 34.1 ğ‘  = 4.3 Using the ğ‘  as an unbiased estimator of ğœ we obtain the 95% CI of: (Use the t distribution)

<detail>
In the PCV example, ğ‘£ = 25 âˆ’ 1 = 24. Using the ğ‘¡ distribution with 24 ğ‘‘ğ‘“, the 95% CI is: Alpha divided by 2= 0.025: using this check for multiplier of t at 24 df
at alpha/2 from t distribution table; you get a multiplier of 2.064 (slightly larger than 1.96 hence the CI obtained is slightly wider than using Z score)

<div style={{ display: 'flex', justifyContent: 'center' }}>
  <img src="https://i.imgur.com/xlpHqKs.png" alt="t-test" />
</div>

Interpretation: 95% confidence that true mean PCV in this population lies between 32.3 and 35.9

</detail>

## Significance test/hypothesis test for a single mean

We may wish to test a specific hypothesis about the pop mean ğœ‡

**Example:** if data on 4yr children in USA indicate a mean PCV of 37.1 we may test whether our sample data (West African) are consistent with the ğ»<sub>0</sub>: First state the hypothesis (using population parameters): in this case itâ€™s 2 sided;

- ğ»<sub>0</sub>: ğœ‡ = ğœ‡0 = 37.1 (W. African statistics = null value US)
- ğ»<sub>ğ‘</sub>: ğœ‡ â‰  ğœ‡0

:::warning tip
You only state hypothesis when running hypothesis/significance test. Confidence interval & hypothesis test are complementary. Both lead to similar conclusion.
:::

One approach to test the hypothesis is to see whether the 95% CI includes the hypothesized value (37.1) â€“ it doesnâ€™t (compare 32.3- 35.9; some evidence against the ğ»0)

More objectively we use a _significance test_ and examine the _ğ‘ƒ-value_ (run the test)

N/B: Every significance test has the difference between what is to be compared in the numerator (eg. the difference between the two means) and denominator is SEM

<div style={{ display: 'flex', justifyContent: 'center' }}>
  <img src="https://i.imgur.com/9GzCRgE.png" alt="z-test" />
</div>

From the ğ‘ ğ‘¡ğ‘ğ‘ğ‘™ğ‘’ğ‘  we get the ğ‘ƒ-value using the Z score of 3.49 (ignore the negative): Read off; 0.00024 (a probability on tail); remember rejection region is in the 5% area. 2 Ã— 0.00024 = ğŸ.ğŸğŸğŸğŸ’ğŸ– (note this is less than 0.05; reject) (NB: ğ‘ƒ-value is normally one-tailed so multiply the resulting probability by 2)

Interpretation: The data provide strong evidence (i.e p value is 0.00048) against ğ‘¯<sub>ğŸ</sub> hence the mean PCV in 4yr old children in the West African village is different from that of children of the same age in the USA (we are rejecting the null hypothesis)

If ğ‘› < 20 then ğ‘¡ ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘Ÿğ‘–ğ‘ğ‘¢ğ‘¡ğ‘–ğ‘œğ‘› is more appropriate:

<div style={{ display: 'flex', justifyContent: 'center' }}>
  <img src="https://i.imgur.com/otl0vKd.png" alt="t-test" />
</div>

From ğ‘¡ ğ‘¡ğ‘ğ‘ğ‘™ğ‘’ğ‘  3.49 falls between 3.467 & 3.745 and df is (25-1 = 24) hence: read off value between 0.001 and 0.0005: to get p value; (0.001 x 2), (0.0005 x 2), thus ğŸ. ğŸğŸğŸ < ğ‘· < ğŸ.ğŸğŸğŸ (> probability of 3.467 and < 3.745): the p value lies in rejection region (< 0.05)

## Sampling distribution of a proportion

Example: In a survey of 335 men attending a health centre in Guilford (UK), 127 (37.9%) men said they were current smokers How we can we use the result from the above sample to say something about the population which it represents? â€“ we use the concept of sampling distribution.  
Suppose we repeatedly took a new sample of 335 men from this health centre (assuming a large no. of men are registered there) and calculated the proportion who smoked & then created a histogram of these values â€“ histogram would represent the sampling distribution of the proportion:

<div style={{ display: 'flex', justifyContent: 'center' }}>
  <img src="https://i.imgur.com/6UyxSOq.png" alt="proportion" />
</div>

In practice we only conduct one survey from which to estimate ğ‘. Is ğ‘ close to **ğœ‹** (population proportion) or is it very different from **ğœ‹**?

In any random sample thereâ€™s some sampling variation in ğ‘ (sampling error) so that the larger the ğ‘› the smaller the sampling variation

The sampling variation of a proportion is described by its **standard error of the proportion**:

<div style={{ display: 'flex', justifyContent: 'center' }}>
  <img src="https://i.imgur.com/Fs34Xap.png" alt="proportion" />
</div>

The ğ‘†ğ¸ of a proportion is a measure of how far our observed proportion **ğ‘** differs from the true pop proportion **ğœ‹**

In the previous UK example, the estimated ğ‘†ğ¸ of the proportion of smokers is:

<div style={{ display: 'flex', justifyContent: 'center' }}>
  <img src="https://i.imgur.com/x8JWeBl.png" alt="proportion" />
</div>

To estimate the confidence interval of possible values within which the true pop proportion ğœ‹ lies we compute the CI: (conditions to use Z score should be met)

<div style={{ display: 'flex', justifyContent: 'center' }}>
  <img src="https://i.imgur.com/S2u5Dlh.png" alt="proportion" />
</div>

Interpretation: We are 95% confident that the true percentage of smokers in Guilford, UK lies between 32.7% and 43.1%

### Significance test/hypothesis test for a single proportion

For this ğ‘ test, again the conditions below must be satisfied: ğ‘›ğ‘ â‰¥ 5 and ğ‘›(1âˆ’ğ‘) â‰¥ 5 ( for a Z score to be used for a proportion these conditions must be met; z-test for a single proportion)

<div style={{ display: 'flex', justifyContent: 'center' }}>
  <img
    src="https://i.imgur.com/XSvsY5h.png"
    alt="z-test for a single proportion"
  />
</div>
